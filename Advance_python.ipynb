{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMtjXcgNuP/0Z1DInFc1iBj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tuhin04/Advance-python-/blob/main/Advance_python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Create a file that contains 1000 lines of random strings.**"
      ],
      "metadata": {
        "id": "SKC7UB1b0tZV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8AwSAtF0qJz"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import string\n",
        "\n",
        "# Generate a random string\n",
        "def generate_random_string(length):\n",
        "    return ''.join(random.choices(string.ascii_letters + string.digits, k=length))\n",
        "\n",
        "# Number of lines and length of each string\n",
        "num_lines = 1000\n",
        "string_length = 10\n",
        "\n",
        "# Generate random strings and write them to a file\n",
        "with open('random_strings.txt', 'w') as file:\n",
        "    for _ in range(num_lines):\n",
        "        random_string = generate_random_string(string_length)\n",
        "        file.write(random_string + '\\n')\n",
        "\n",
        "print(\"File created: random_strings.txt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Create a file that contains multiple lines of random strings and file size must be 5 MB**"
      ],
      "metadata": {
        "id": "SIrB9RS02gyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import string\n",
        "import os\n",
        "\n",
        "# Generate a random string\n",
        "def generate_random_string(length):\n",
        "    return ''.join(random.choices(string.ascii_letters + string.digits, k=length))\n",
        "\n",
        "# Target file size in bytes (5 MB)\n",
        "target_file_size = 5 * 1024 * 1024\n",
        "\n",
        "# Open the file in write mode\n",
        "with open('random_strings.txt', 'w') as file:\n",
        "    while os.path.getsize('random_strings.txt') < target_file_size:\n",
        "        random_string = generate_random_string(100)  # Choose a desired string length\n",
        "        file.write(random_string + '\\n')\n",
        "\n",
        "print(\"File created: random_strings.txt\")\n"
      ],
      "metadata": {
        "id": "tJia2oZE2mnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Create 10 files that contains multiple lines of random strings and file size of each file must be 5 MB.**"
      ],
      "metadata": {
        "id": "n3wVVHY-2wzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import string\n",
        "import os\n",
        "\n",
        "# Generate a random string\n",
        "def generate_random_string(length):\n",
        "    return ''.join(random.choices(string.ascii_letters + string.digits, k=length))\n",
        "\n",
        "# Target file size in bytes (5 MB)\n",
        "target_file_size = 5 * 1024 * 1024\n",
        "\n",
        "# Number of files to create\n",
        "num_files = 10\n",
        "\n",
        "# Open multiple files and write random strings\n",
        "for file_number in range(num_files):\n",
        "    file_name = f'random_strings_{file_number}.txt'\n",
        "    with open(file_name, 'w') as file:\n",
        "        while os.path.getsize(file_name) < target_file_size:\n",
        "            random_string = generate_random_string(100)  # Choose a desired string length\n",
        "            file.write(random_string + '\\n')\n",
        "\n",
        "    print(f\"File created: {file_name}\")\n"
      ],
      "metadata": {
        "id": "GYswE37n234t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Create 5 files of size 1GB, 2GB, 3GB, 4GB and 5GB; file contains multiple lines of random strings.**"
      ],
      "metadata": {
        "id": "kfAT_99T2-wn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import string\n",
        "\n",
        "# Generate a random string\n",
        "def generate_random_string(length):\n",
        "    return ''.join(random.choices(string.ascii_letters + string.digits, k=length))\n",
        "\n",
        "# Target file sizes in bytes\n",
        "target_file_sizes = [1_073_741_824, 2_147_483_648, 3_221_225_472, 4_294_967_296, 5_368_709_120]  # 1GB, 2GB, 3GB, 4GB, 5GB\n",
        "\n",
        "# String length and number of lines (adjust as needed)\n",
        "string_length = 100\n",
        "num_lines = 10000\n",
        "\n",
        "# Create files of different sizes\n",
        "for i, size in enumerate(target_file_sizes):\n",
        "    file_name = f\"random_strings_{i+1}GB.txt\"\n",
        "    bytes_written = 0\n",
        "\n",
        "    with open(file_name, 'w') as file:\n",
        "        while bytes_written < size:\n",
        "            random_string = generate_random_string(string_length) + '\\n'\n",
        "            file.write(random_string)\n",
        "            bytes_written += len(random_string.encode())\n",
        "\n",
        "    print(f\"File created: {file_name}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAHBN4LI3Dfc",
        "outputId": "1649c5cc-1bc9-4bb0-881c-fb486b051647"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File created: random_strings_1GB.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Convert all the files of Q4 into upper case one by one.**"
      ],
      "metadata": {
        "id": "EKRVupnm3jmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of file names\n",
        "file_names = ['random_strings_1GB.txt', 'random_strings_2GB.txt', 'random_strings_3GB.txt',\n",
        "              'random_strings_4GB.txt', 'random_strings_5GB.txt']\n",
        "\n",
        "# Convert each file to uppercase\n",
        "for file_name in file_names:\n",
        "    # Read the file content\n",
        "    with open(file_name, 'r') as file:\n",
        "        content = file.read()\n",
        "\n",
        "    # Convert the content to uppercase\n",
        "    content_upper = content.upper()\n",
        "\n",
        "    # Write the uppercase content back to the file\n",
        "    with open(file_name, 'w') as file:\n",
        "        file.write(content_upper)\n",
        "\n",
        "    print(f\"File converted to uppercase: {file_name}\")\n"
      ],
      "metadata": {
        "id": "rGdySDHi3s6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Convert all the files of Q4 into upper case parallel using multi-threading.**"
      ],
      "metadata": {
        "id": "sTdmNZBR32cT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import concurrent.futures\n",
        "\n",
        "# List of file names\n",
        "file_names = ['random_strings_1GB.txt', 'random_strings_2GB.txt', 'random_strings_3GB.txt',\n",
        "              'random_strings_4GB.txt', 'random_strings_5GB.txt']\n",
        "\n",
        "# Function to convert a file to uppercase\n",
        "def convert_to_uppercase(file_name):\n",
        "    # Read the file content\n",
        "    with open(file_name, 'r') as file:\n",
        "        content = file.read()\n",
        "\n",
        "    # Convert the content to uppercase\n",
        "    content_upper = content.upper()\n",
        "\n",
        "    # Write the uppercase content back to the file\n",
        "    with open(file_name, 'w') as file:\n",
        "        file.write(content_upper)\n",
        "\n",
        "    return file_name\n",
        "\n",
        "# Convert files to uppercase in parallel\n",
        "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "    results = executor.map(convert_to_uppercase, file_names)\n",
        "\n",
        "# Print the converted files\n",
        "for result in results:\n",
        "    print(f\"File converted to uppercase: {result}\")\n"
      ],
      "metadata": {
        "id": "4aRHYc3d4EKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q7. WAP to automatically download 10 images of cat from “Google Images”. [Hint: Find the package from\n",
        "pypi.org and use it**"
      ],
      "metadata": {
        "id": "v0CFCTlh4LEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google_images_search import GoogleImagesSearch\n",
        "\n",
        "# Set up the Google Images Search API\n",
        "gis = GoogleImagesSearch('YOUR_API_KEY', 'YOUR_CX')\n",
        "\n",
        "# Set the search parameters\n",
        "search_params = {\n",
        "    'q': 'cat',\n",
        "    'num': 10,\n",
        "    'safe': 'high',\n",
        "    'fileType': 'jpg'\n",
        "}\n",
        "\n",
        "# Perform the search and download the images\n",
        "gis.search(search_params=search_params, path_to_dir='cat_images')\n",
        "\n",
        "print(\"Images downloaded successfully.\")\n"
      ],
      "metadata": {
        "id": "2plj2Siu4PIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. WAP to automatically download 10 videos of “Machine Learning” from “Youtube.com”. [Hint: Find the\n",
        "package from pypi.org and use it]**"
      ],
      "metadata": {
        "id": "Mt5PqHh04SK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pytube\n",
        "\n",
        "# Set the search query and maximum number of videos to download\n",
        "search_query = 'Machine Learning'\n",
        "max_videos = 10\n",
        "\n",
        "# Perform the search and download the videos\n",
        "results = pytube.Search(search_query).results[:max_videos]\n",
        "for video in results:\n",
        "    video.streams.get_highest_resolution().download(output_path='videos')\n",
        "\n",
        "print(\"Videos downloaded successfully.\")\n"
      ],
      "metadata": {
        "id": "NyhYXj7r4Yal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. Convert all the videos of Q8 and convert it to audio. [Hint: Find the package from pypi.org and use it]**"
      ],
      "metadata": {
        "id": "j75H92eN4ejP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from moviepy.editor import VideoFileClip\n",
        "\n",
        "# Convert each video to audio\n",
        "for i in range(1, 11):\n",
        "    video_path = f\"videos/video{i}.mp4\"\n",
        "    audio_path = f\"audios/audio{i}.mp3\"\n",
        "\n",
        "    video = VideoFileClip(video_path)\n",
        "    audio = video.audio\n",
        "\n",
        "    audio.write_audiofile(audio_path)\n",
        "\n",
        "print(\"Videos converted to audio successfully.\")\n"
      ],
      "metadata": {
        "id": "ejxSKWjx4klT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10. Create an automated pipeline using multi-threading for:\n",
        "“Automatic Download of 100 Videos from YouTube” → “Convert it to Audio”**"
      ],
      "metadata": {
        "id": "jGDZsLAN4n81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pytube\n",
        "from moviepy.editor import VideoFileClip\n",
        "import concurrent.futures\n",
        "\n",
        "# Set the search query and maximum number of videos to download\n",
        "search_query = 'Machine Learning'\n",
        "max_videos = 100\n",
        "\n",
        "# Function to download and convert a video\n",
        "def download_and_convert_video(video_url):\n",
        "    video = pytube.YouTube(video_url)\n",
        "    video.streams.get_highest_resolution().download(output_path='videos')\n",
        "\n",
        "    video_path = f\"videos/{video.streams[0].default_filename}\"\n",
        "    audio_path = f\"audios/{video.streams[0].default_filename[:-4]}.mp3\"\n",
        "\n",
        "    video = VideoFileClip(video_path)\n",
        "    audio = video.audio\n",
        "    audio.write_audiofile(audio_path)\n",
        "\n",
        "    return video_url\n",
        "\n",
        "# Search for videos and download them in parallel\n",
        "results = pytube.Search(search_query).results[:max_videos]\n",
        "\n",
        "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "    results = executor.map(download_and_convert_video, [video.watch_url for video in results])\n",
        "\n",
        "# Print the downloaded and converted videos\n",
        "for result in results:\n",
        "    print(f\"Video downloaded and converted: {result}\")\n"
      ],
      "metadata": {
        "id": "bwp3Yr1J4sCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**11. Create an automated pipeline using multi-threading for: “Automatic Download of 500 images of Dog from\n",
        "GoogleImages” → “Rescale it to 50%**"
      ],
      "metadata": {
        "id": "y-u0Yhqb4uYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google_images_search import GoogleImagesSearch\n",
        "from PIL import Image\n",
        "import concurrent.futures\n",
        "\n",
        "# Set up the Google Images Search API\n",
        "gis = GoogleImagesSearch('YOUR_API_KEY', 'YOUR_CX')\n",
        "\n",
        "# Set the search parameters\n",
        "search_params = {\n",
        "    'q': 'dog',\n",
        "    'num': 500,\n",
        "    'safe': 'high',\n",
        "    'fileType': 'jpg'\n",
        "}\n",
        "\n",
        "# Function to download and rescale an image\n",
        "def download_and_rescale_image(image_url):\n",
        "    gis.download(image_url, path_to_dir='dog_images')\n",
        "    image_path = f\"dog_images/{image_url.split('/')[-1]}\"\n",
        "\n",
        "    image = Image.open(image_path)\n",
        "    image_rescaled = image.resize((int(image.width * 0.5), int(image.height * 0.5)))\n",
        "    image_rescaled.save(image_path)\n",
        "\n",
        "    return image_url\n",
        "\n",
        "# Search for images and download them in parallel\n",
        "gis.search(search_params=search_params, path_to_dir='dog_images')\n",
        "results = [image.url for image in gis.results()]\n",
        "\n",
        "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "    results = executor.map(download_and_rescale_image, results)\n",
        "\n",
        "# Print the downloaded and rescaled images\n",
        "for result in results:\n",
        "    print(f\"Image downloaded and rescaled: {result}\")\n"
      ],
      "metadata": {
        "id": "QR4SAd754yrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3kkM70l549hs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}